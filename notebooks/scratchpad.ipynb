{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_ACCESS_TOKEN = os.getenv(\"GITHUB_ACCESS_TOKEN\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# credential json not required if you are working within vertex AI workbench\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/Transcendent/fleet-anagram-244304-7dafcc771b2f.json\"\n",
    "\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") # only if you are using text embedding model from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_events(username):\n",
    "    url = f\"https://api.github.com/users/{username}/events\"\n",
    "    headers = {\"Authorization\": f\"token {GITHUB_ACCESS_TOKEN}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "    events_data = response.json()\n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the chat model\n",
    "# llm = ChatVertexAI(model=\"gemini-1.5-flash\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_event(event, llm):\n",
    "    event_details = {\n",
    "        \"type\": event[\"type\"],\n",
    "        \"repo\": event[\"repo\"][\"name\"],\n",
    "        \"actor\": event[\"actor\"][\"login\"],\n",
    "        \"created_at\": event[\"created_at\"],\n",
    "        \"commit_message\": event[\"payload\"][\"commits\"][0][\"message\"] if event[\"type\"] == \"PushEvent\" else None  \n",
    "    }\n",
    "    \n",
    "    # Convert event details to a string format\n",
    "    event_details_str = \"\\n\".join(f\"{key}: {value}\" for key, value in event_details.items() if value is not None)\n",
    "    \n",
    "    prompt = f\"Summarize this GitHub event in one sentence:\\n{event_details_str}\"\n",
    "    \n",
    "    # Generate response from the language model\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Extract the content from the AI's response\n",
    "    summary = response.content\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data = get_github_events(\"whanyu1212\")\n",
    "event_summaries = []\n",
    "for event in events_data:\n",
    "    if event['type'] in ['PushEvent', 'WatchEvent']:\n",
    "        summary = summarize_event(event,llm)\n",
    "        event_summaries.append({\"type\": event['type'], \"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_string = json.dumps(event_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(f\"Tabulate the event type and counts from {event_summaries}. Summarize which repository has the most activity\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Here is the tabulated count of event types:\n",
       "\n",
       "| Event Type   | Count |\n",
       "|--------------|-------|\n",
       "| PushEvent    | 9     |\n",
       "| WatchEvent   | 14    |\n",
       "\n",
       "### Summary of Repository Activity\n",
       "\n",
       "#### PushEvent Details:\n",
       "- Repository \"whanyu1212/Transcendent\" had 7 PushEvents:\n",
       "  - added an example on retrieving github events.\n",
       "  - minor font changes.\n",
       "  - add on to the existing todo list.\n",
       "  - minor font changes.\n",
       "  - added a simple to do list in readme.\n",
       "  - README draft.\n",
       "  - setting up pre-commit hooks.\n",
       "  - initial dependencies commit.\n",
       "\n",
       "- Repository \"whanyu1212/leetcode-kattis-practice\" had 3 PushEvents:\n",
       "  - adding the minion game and mentioning an unresolved runtime error in some test cases.\n",
       "  - addressing a directory issue.\n",
       "  - restructuring the folders.\n",
       "\n",
       "#### WatchEvent Details:\n",
       "- Various repositories were watched/starred, with no single repository standing out for most watch events.\n",
       "\n",
       "### Repository with the Most Activity:\n",
       "The repository with the most activity is \"whanyu1212/Transcendent\" with a total of 7 PushEvents.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
